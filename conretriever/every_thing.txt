git config --global http.proxy http://10.176.52.116:7890
git config --global https.proxy http://10.176.52.116:7890
export https_proxy=http://10.176.52.116:7890 http_proxy=http://10.176.52.116:7890 all_proxy=socks5://10.176.52.116:7891

git config --global --unset http.proxy
git config --global --unset https.proxy

git config --global http.proxy http://10.162.249.97:10809
git config --global https.proxy http://10.162.249.97:10809
export https_proxy=http://10.162.249.97:10809 http_proxy=http://10.162.249.97:10809 all_proxy=socks5://10.162.249.97:10808

git config --global http.proxy http://10.230.33.64:10809
git config --global https.proxy http://10.230.33.64:10809
export https_proxy=http://10.230.33.64:10809 http_proxy=http://10.230.33.64:10809 all_proxy=socks5://10.230.33.64:10808


torchrun --nproc_per_node=2 --master_port=20001 ./fastchat/train/train_mem.py \
    --model_name_or_path /cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/public/models/llama2_hf \
    --data_path data/dummy_conversation.json \
    --bf16 True \
    --output_dir output_vicuna \
    --num_train_epochs 30 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 16 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 1200 \
    --save_total_limit 10 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --fsdp "full_shard auto_wrap" \
    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \
    --tf32 True \
    --model_max_length 2048 \
    --gradient_checkpointing True \
    --lazy_preprocess True


torchrun --nproc_per_node=2 --master_port=20001 ./mine/train_retrieval.py \
    --model_name_or_path /cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/public/models/mistral_7b_instruct \
    --data_path data/dummy_conversation.json \
    --bf16 True \
    --output_dir output_vicuna \
    --num_train_epochs 30 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 16 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 1200 \
    --save_total_limit 10 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --fsdp "full_shard auto_wrap" \
    --fsdp_transformer_layer_cls_to_wrap 'MistralDecoderLayer' \
    --tf32 True \
    --model_max_length 2048 \
    --gradient_checkpointing True \
    --lazy_preprocess True


torchrun --nproc_per_node=2 --master_port=20001 ./mine/train_retrieval.py \
    --model_name_or_path /cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/public/models/mistral_7b_instruct \
    --data_path /cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/dataset/msmarco_train_small.jsonl \
    --bf16 True \
    --output_dir output_representation_token_num_8 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 512 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 200 \
    --save_total_limit 10 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --fsdp "shard_grad_op auto_wrap" \
    --fsdp_transformer_layer_cls_to_wrap 'MistralDecoderLayer' \
    --tf32 True \
    --model_max_length 512 \
    --gradient_checkpointing True \
    --lazy_preprocess True \
    --n_hard_negative 0 \
    --representation_token_num 8 \
    --remove_unused_columns False \
    --chunk_sizes 16 \
    --dataloader_num_workers 4

torchrun --nproc_per_node=1 --master_port=20001 ./mine/train_retrieval.py \
    --model_name_or_path /cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/public/models/mistral_7b_instruct \
    --data_path /cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/dataset/msmarco_train_small.jsonl \
    --bf16 True \
    --output_dir checkpoint_dir/tmp \
    --num_train_epochs 1 \
    --per_device_train_batch_size 64 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 2 \
    --save_total_limit 10 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --fsdp "shard_grad_op auto_wrap" \
    --fsdp_transformer_layer_cls_to_wrap 'MistralDecoderLayer' \
    --tf32 True \
    --model_max_length 512 \
    --gradient_checkpointing True \
    --lazy_preprocess True \
    --n_hard_negative 0 \
    --representation_token_num 8 \
    --remove_unused_columns False \
    --chunk_sizes 16 \
    --dataloader_num_workers 4



   shard_grad_op
   --fsdp "shard_grad_op auto_wrap" \


scp -P 30299 -r msmarco_train.jsonl lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/dataset/

--warmup_ratio 0.03 \

scp -P 30138 -r lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/mycode/fast_chat/checkpoint_dir/output_representation_token_num_16/checkpoint-500 checkpoint_dir/


rsync -avz --progress  --exclude "optimizer.bin" --exclude "pytorch_model_fsdp.bin" -e "ssh -p 30138" lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/mycode/fast_chat/checkpoint_dir/output_representation_token_num_1/checkpoint-500 checkpoint_dir/output_representation_token_num_1/


deepspeed --include localhost:4,5 \

deepspeed --include localhost:4,5 \
fastchat/train/train_lora.py \
    --model_name_or_path /remote-home/share/models/mistral_7b_base \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --data_path ./data/dummy_conversation.json \
    --bf16 True \
    --output_dir ./tmp_checkpoints \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 20 \
    --save_total_limit 100 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --tf32 True \
    --model_max_length 2048 \
    --q_lora False \
    --deepspeed playground/my_config_zero2.json


    --fsdp "full_shard auto_wrap" \
    --fsdp "shard_grad_op auto_wrap" \


    --model_name_or_path /cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/public/models/mistral_7b_base \
    --data_path /cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/dataset/with_hn/tmp_msmarco_with_hn_bm25_bge.jsonl \


    --model_name_or_path /remote-home/share/models/mistral_7b_base \
    --data_path /remote-home/xnli/mycode/msmarco_data/with_hn/tmp_msmarco_with_hn_bm25_bge.jsonl \


    scp -P 30123 -r /remote-home/xnli/mycode/msmarco_data/with_hn/tmp_msmarco_with_hn.jsonl lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/dataset/with_hn

    scp -P 30123 -r /remote-home/xnli/mycode/msmarco_data/with_hn/tmp_msmarco_with_hn_bm25.jsonl  lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/dataset/with_hn/


srun -p a800 --gres=gpu:1 --cpus-per-task=12 --mem-per-cpu=4G --pty bash


    scp -P 30123 -r /remote-home/xnli/.cache/huggingface/datasets/BeIR/nq  lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/.cache/huggingface/datasets/BeIR

    scp -P 30123 -r /remote-home/xnli/mycode/fast_chat/checkpoint_dir/msmarco_with_hn_no_lora_lr_1e_5_new  lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/mycode/fast_chat/checkpoint_dir

    scp -P 30281 -r /remote-home/xnli/.cache/huggingface/datasets/BeIR/*.zip lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/.cache/huggingface/datasets/BeIR/

    scp -P 30281 -r checkpoint_dir/msmarco_with_hn_use_lora_lr_2e_4_with_weight_decay lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/mycode/fast_chat/checkpoint_dir

    scp -P 30281 -r /remote-home/share/models/e5_mistral_7b lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/public/models/e5_mistral_7b

    /remote-home/share/models/e5_mistral_7b

        scp -P 30281 -r /remote-home/share/models/llama_v2_hf/7b lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/public/models/llama_2_7b_base

    scp -P 30281 -r /remote-home/xnli/mycode/tevatron/examples/repllama/model_repllama/checkpoint-1000 lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/mycode/tevatron/examples/repllama/model_llama

    scp -P 30281 -r /remote-home/xnli/mycode/tevatron/examples/repllama/model_repllama/checkpoint-1000 lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/mycode/tevatron/examples/repllama/model_llama

    scp -P 30281 -r checkpoint_dir/msmarco_with_hn_no_lora_lr_1e_5_xueguang_recipe_1_at_2024_1_21 lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/mycode/fast_chat/checkpoint_dir

srun -p a800 --nodelist=slurmd-7 --gres=gpu:3 --cpus-per-task=36 --mem-per-cpu=4G --pty bash

    scp -P 30281 -r /remote-home/xnli/mycode/msmarco_data/with_hn/tmp_msmarco_with_hn_xueguang.jsonl lxn_20110240012@10.193.2.99:/cpfs01/projects-HDD/cfff-6ef6b3b71ce2_HDD/lxn_20110240012/dataset/with_hn